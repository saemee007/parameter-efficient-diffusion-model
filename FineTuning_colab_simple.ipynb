{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDwM5xLRggN5"
   },
   "source": [
    "# Fine-tuning of Stable Diffusion models\n",
    "\n",
    "A notebook for training Stable Diffusion models using Dreambooth or Low-rank Adaptation (LoRA) approaches.\n",
    "\n",
    "Tested with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) and [Stable Diffusion v2-base](https://huggingface.co/stabilityai/stable-diffusion-2-base).\n",
    "\n",
    "Stay up-to-date on [Github](https://github.com/brian6091/Dreambooth), and leave an [issue](https://github.com/brian6091/Dreambooth/issues) if you run into problems.\n",
    "\n",
    "[![Brian6091's GitHub stats](https://github-readme-stats.vercel.app/api?username=brian6091&hide=contribs&theme=onedark&show_icons=true)](https://github.com/brian6091/Dreambooth)\n",
    "\n",
    "[<a href=\"https://www.buymeacoffee.com/jvsurfsqv\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" height=\"40px\" width=\"140px\" alt=\"Buy Me A Coffee\"></a>](https://www.buymeacoffee.com/jvsurfsqv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LouTFfVYhRei"
   },
   "source": [
    "# Install dependencies (takes about 1 minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmsR_IPcvp7v"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !cd content/\n",
    "# !git clone https://github.com/brian6091/Dreambooth --branch main --single-branch\n",
    "# !pip install -r \"Dreambooth/requirements.txt\"\n",
    "# !pip install -U --pre triton\n",
    "# !pip install torchinfo\n",
    "\n",
    "# !git clone https://github.com/brian6091/lora --branch v0.0.5 --single-branch\n",
    "# !python -m pip install content/lora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tfenRTEQz_R"
   },
   "outputs": [],
   "source": [
    "#@title xformers\n",
    "#%%capture\n",
    "\n",
    "# !nvidia-smi -L\n",
    "\n",
    "# Tested with Tesla T4 and A100 GPUs\n",
    "# !pip install xformers==0.0.16rc425\n",
    "# May complain about some incompatibilities, which are resolved by upgrading the following:\n",
    "#!pip install -U --pre torchvision\n",
    "#!pip install -U --pre torchtext\n",
    "#!pip install -U --pre torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C3LrrpBvYn-"
   },
   "source": [
    "# Which model to train from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YI6dHfQ8iqMg"
   },
   "outputs": [],
   "source": [
    "#@title ## Name or path to initial model and VAE\n",
    "#@markdown Obligatory (e.g., runwayml/stable-diffusion-v1-5, stabilityai/stable-diffusion-2-base, or full path to model in diffusers format)\n",
    "MODEL_NAME_OR_PATH = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Optional (e.g., stabilityai/sd-vae-ft-mse), leaving empty will default to VAE packaged with the model\n",
    "VAE_NAME_OR_PATH = \"\" #@param {type:\"string\"}\n",
    "#if VAE_NAME_OR_PATH==\"\":\n",
    "#  VAE_NAME_OR_PATH = None\n",
    "\n",
    "#@markdown (Not yet implemented), leaving empty will default to text encoder packaged with the model\n",
    "TEXT_ENCODER_NAME_OR_PATH = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BGzA0N0C0pDB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/saemeechoi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#@title ## Hugging Face ðŸ¤— credentials\n",
    "\n",
    "#@markdown If initiating training from official stable diffusion checkpoints (e.g., [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)), you must accept the license before using the model. You'll need a [ðŸ¤— Hugging Face](https://huggingface.co/) account to do so, after which you can [generate a login token](https://huggingface.co/settings/tokens) and paste it here.\n",
    "from huggingface_hub import login\n",
    "\n",
    "HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n",
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2wBnGW_v00va"
   },
   "outputs": [],
   "source": [
    "# #@title ## Mount Google Drive if initial model stored there (or you want to direct outputs there)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxP_d_n4mW2_"
   },
   "source": [
    "# Set up experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "E1cJJ-P8jPhx"
   },
   "outputs": [],
   "source": [
    "#@title ## Training parameters\n",
    "\n",
    "import os\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "#@markdown Unique token for specific subject\n",
    "INSTANCE_TOKEN= \"raretoken\" #@param{type: 'string'}\n",
    "\n",
    "#@markdown Use image captions? Captions can be either the image filename, or a separate text file (that must be named identically to the image but w/ extension .txt). If a separate .txt file exists, filename is ignored.\n",
    "USE_IMAGE_CAPTIONS = False #@param {type:\"boolean\"}\n",
    "USE_IMAGE_CAPTIONS_FLAG = \"\"\n",
    "if USE_IMAGE_CAPTIONS:\n",
    "  USE_IMAGE_CAPTIONS_FLAG='--use_image_captions'\n",
    "\n",
    "#@markdown Path to instance images. Filenames are irrelevant, unless images are captioned *and* captions are not separate textfiles, in which case INSTANCE_TOKEN should appear in relevant filenames as part of the caption.\n",
    "INSTANCE_DIR=\"content/InstanceImages\" #@param{type: 'string'}\n",
    "\n",
    "RESOLUTION = 512 #@param{type: 'number'}\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1 #@param{type: 'number'}\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 1  #@param{type: 'number'}\n",
    "\n",
    "GRADIENT_CHECKPOINTING = True #@param {type:\"boolean\"}\n",
    "GRADIENT_CHECKPOINTING_FLAG=\"\"\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "  GRADIENT_CHECKPOINTING_FLAG='--gradient_checkpointing'\n",
    "\n",
    "ENABLE_PRIOR_PRESERVATION = True #@param {type:\"boolean\"}\n",
    "ENABLE_PRIOR_PRESERVATION_FLAG=\"\"\n",
    "if ENABLE_PRIOR_PRESERVATION:\n",
    "  ENABLE_PRIOR_PRESERVATION_FLAG='--with_prior_preservation'\n",
    "\n",
    "#@markdown Prior loss weight. Note that if you set this to 0, but enable prior preservation and provide a CLASS_DIR, you can still monitor class loss.\n",
    "PRIOR_LOSS_WEIGHT = 1.0 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown If using prior preservation, specify a path to class images\n",
    "CLASS_DIR=\"content/RegularizationImages\" #@param{type: 'string'}\n",
    "if (CLASS_DIR !=\"\") and os.path.exists(str(CLASS_DIR)):\n",
    "  CLASS_DIR=CLASS_DIR\n",
    "elif (CLASS_DIR !=\"\") and not os.path.exists(str(CLASS_DIR)):\n",
    "  CLASS_DIR=input('\u001b[1;31mThe folder specified does not exist, use the colab file explorer to copy the path :')\n",
    "\n",
    "#@markdown Prompt for prior preservation class (e.g., 'person', 'a photo of a man', 'dog'). Ignored if USE_IMAGE_CAPTIONS checked.\n",
    "CLASS_PROMPT=\"a photo of a person\" #@param {type:\"string\"}\n",
    "#@markdown Instance prompt, {SKS} will be automatically replaced by INSTANCE_TOKEN defined above.  Ignored if USE_IMAGE_CAPTIONS checked.\n",
    "INSTANCE_PROMPT=\"a photo of {SKS} person\" #@param {type:\"string\"}\n",
    "INSTANCE_PROMPT=INSTANCE_PROMPT.replace(\"{SKS}\",INSTANCE_TOKEN)\n",
    "\n",
    "#@markdown Specify the number of class images used if prior preservation is enabled. If there are not enough images in CLASS_DIR (or CLASS_DIR is empty), additional images will be generated.\n",
    "MIN_NUM_CLASS_IMAGES=1500 #@param{type: 'number'}\n",
    "\n",
    "#@markdown Batch size for generating class images\n",
    "SAMPLE_BATCH_SIZE = 1 #@param{type: 'number'}\n",
    "\n",
    "#@markdown Number of training iterations, e.g., # instance images * 100\n",
    "STEPS = 10000 #@param{type: 'number'}\n",
    "\n",
    "#@markdown Random number generator seed\n",
    "SEED = 1275017 #@param{type: 'number'}\n",
    "\n",
    "#@markdown Enable text encoder training?\n",
    "TRAIN_TEXT_ENCODER = False #@param{type: 'boolean'}\n",
    "TRAIN_TEXT_ENCODER_FLAG=\"\"\n",
    "if TRAIN_TEXT_ENCODER:\n",
    "  TRAIN_TEXT_ENCODER_FLAG=\"--train_text_encoder\"\n",
    "\n",
    "#@markdown ## ADAM optimizer settings\n",
    "\n",
    "#@markdown Use 8-bit ADAM\n",
    "USE_8BIT_ADAM = True #@param {type:\"boolean\"}\n",
    "USE_8BIT_ADAM_FLAG=\"\"\n",
    "if USE_8BIT_ADAM:\n",
    "  USE_8BIT_ADAM_FLAG='--use_8bit_adam'\n",
    "\n",
    "#@markdown The exponential decay rate for the 1st moment estimates (the beta1 parameter for the Adam optimizer).\n",
    "ADAM_BETA1 = 0.9 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown The exponential decay rate for the 2nd moment estimates (the beta2 parameter for the Adam optimizer).\n",
    "ADAM_BETA2 = 0.999 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown Weight decay magnitude for the Adam optimizer.\n",
    "ADAM_WEIGHT_DECAY = 1e-2 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown Epsilon value for the Adam optimizer.\n",
    "ADAM_EPSILON = 1e-08 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown \"fp16\", \"bf16\", or \"no\" according to available VRAM\n",
    "MIXED_PRECISION = \"fp16\" #@param{type: 'string'}\n",
    "\n",
    "#@markdown ## Learning rate parameters\n",
    "LR_SCHEDULE = \"cosine\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
    "LR = 2e-6 #@param{type: 'number'}\n",
    "#@markdown If training the text encoder, a different learning rate can be applied\n",
    "LR_TEXT_ENCODER = 5e-5 #@param{type: 'number'}\n",
    "LR_WARMUP_STEPS = 50 #@param{type: 'number'}\n",
    "#@markdown Applies only for cosine_with_restarts schedule\n",
    "LR_COSINE_NUM_CYCLES = 5 #@param{type: 'number'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zLGpiF7xsLcb"
   },
   "outputs": [],
   "source": [
    "#@title # (Experimental) [Data augmentation](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0/)\n",
    "#@markdown Transformations to apply to images (both instance and class).\n",
    "#@markdown I find this useful to minimize the work of cropping and manually preparing images.\n",
    "#@markdown This may be useful for certain applications, such as training a style, where there may not be a specific subject in each image.\n",
    "#@markdown In this case, I don't crop images, and I enable random cropping, which presents to the network a randomly cropped (RESOLUTION X RESOLUTION) chunk of the original image selected for that iteration.\n",
    "#@markdown AUGMENT_MIN_RESOLUTION allows you to adjust how much of the image you will crop. So if you are training for RESOLUTION=512, setting AUGMENT_MIN_RESOLUTION will give you two crops (on average) for the shortest image dimension.\n",
    "\n",
    "#@markdown Resize image so that smallest dimension = AUGMENT_MIN_RESOLUTION (maintaining aspect ratio). Leave empty to skip.\n",
    "AUGMENT_MIN_RESOLUTION = None #@param{type: 'number'}\n",
    "AUGMENT_MIN_RESOLUTION_FLAG = \"\"\n",
    "if AUGMENT_MIN_RESOLUTION is not None:\n",
    "  AUGMENT_MIN_RESOLUTION = int(AUGMENT_MIN_RESOLUTION)\n",
    "  AUGMENT_MIN_RESOLUTION_FLAG = f\"--augment_min_resolution={AUGMENT_MIN_RESOLUTION}\"\n",
    "\n",
    "#@markdown If not enabled, defaults to center crop (which will do nothing if your images are already square at the RESOLUTION set above).\n",
    "AUGMENT_RANDOM_CROP = False #@param{type: 'boolean'}\n",
    "AUGMENT_CENTER_CROP_FLAG=\"--augment_center_crop\"\n",
    "if AUGMENT_RANDOM_CROP:\n",
    "  AUGMENT_CENTER_CROP_FLAG=\"\"\n",
    "\n",
    "#@markdown Randomly flip image horizontally. Not recommended if asymmetry is important (e.g., faces).\n",
    "AUGMENT_HFLIP = False #@param{type: 'boolean'}\n",
    "AUGMENT_HFLIP_FLAG=\"\"\n",
    "if AUGMENT_HFLIP:\n",
    "  AUGMENT_HFLIP_FLAG=\"--augment_hflip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "LU7NC1Pkr47k"
   },
   "outputs": [],
   "source": [
    "#@title # (Experimental) other training parameters\n",
    "\n",
    "#@markdown ## [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685v2)\n",
    "#@markdown Uses [clonesimo's implementation](https://github.com/cloneofsimo/lora)\n",
    "USE_LORA = False #@param{type: 'boolean'}\n",
    "USE_LORA_FLAG=\"\"\n",
    "if USE_LORA:\n",
    "  USE_LORA_FLAG=\"--use_lora\"\n",
    "\n",
    "#@markdown Rank of LoRA update matrix\n",
    "LORA_RANK = 4 #@param{type: 'number'}\n",
    "\n",
    "#@markdown ## [Drop text-conditioning to improve classifier-free guidance sampling](https://arxiv.org/abs/2207.12598)\n",
    "\n",
    "#@markdown Probability that image (applies to both instance and class images) will be selected for dropout (INSTANCE_PROMPT/CLASS_PROMPT will be replaced with UNCONDITIONAL_PROMPT)\n",
    "CONDITIONING_DROPOUT_PROB = 0.0 #@param{type: 'number'}\n",
    "#@markdown Defaults to an empty prompt. Unsure whether anything else would be useful.\n",
    "UNCONDITIONAL_PROMPT = \" \" #@param{type: 'string'}\n",
    "\n",
    "#@markdown ## Exponentially-weight moving average weights (unet only). Will not run on Tesla T4 (out of memory).\n",
    "USE_EMA = False #@param{type: 'boolean'}\n",
    "USE_EMA_FLAG=\"\"\n",
    "if USE_EMA:\n",
    "  USE_EMA_FLAG=\"--use_ema\"\n",
    "EMA_INV_GAMMA = 1.0 #@param{type: 'number'}\n",
    "EMA_POWER = 0.75 #@param{type: 'number'}\n",
    "EMA_MIN_VALUE = 0 #@param{type: 'number'}\n",
    "EMA_MAX_VALUE = 0.9999 #@param{type: 'number'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lji3GATOYIg_"
   },
   "outputs": [],
   "source": [
    "#@title # Where should outputs get saved?\n",
    "\n",
    "#@markdown Trained models (and intermediates) saved here\n",
    "OUTPUT_DIR=\"content/models/\" #@param{type: 'string'}\n",
    "\n",
    "#@markdown Training logs saved here\n",
    "LOGGING_DIR=\"content/logs/\" #@param{type: 'string'}\n",
    "\n",
    "if not os.path.exists(LOGGING_DIR):\n",
    "  !mkdir -p \"$LOGGING_DIR\"\n",
    "\n",
    "LOG_GPU = True #@param{type: 'boolean'}\n",
    "if LOG_GPU:\n",
    "  LOG_GPU_FLAG=\"--log_gpu\"\n",
    "else:\n",
    "  LOG_GPU_FLAG=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m9wXEuCnXn_0"
   },
   "outputs": [],
   "source": [
    "#@title # Setup saving of intermediate models\n",
    "#@markdown To save intermediate checkpoints, set START_SAVING_FROM_STEP < STEPS\n",
    "\n",
    "#@markdown Number of steps between intermediate saves\n",
    "SAVE_CHECKPOINT_EVERY = 500 #@param{type: 'number'}\n",
    "if SAVE_CHECKPOINT_EVERY==None:\n",
    "  SAVE_CHECKPOINT_EVERY = STEPS+1\n",
    "\n",
    "START_SAVING_FROM_STEP=500 #@param{type: 'number'}\n",
    "if START_SAVING_FROM_STEP==None:\n",
    "  START_SAVING_FROM_STEP=STEPS\n",
    "\n",
    "#@markdown At each intermediate checkpoint, infer this many samples using SAVE_SAMPLE_PROMPT\n",
    "N_SAVE_SAMPLES=2 #@param{type: 'number'}\n",
    "\n",
    "#@markdown {SKS} is automatically replaced by INSTANCE_TOKEN. Give multiple prompts using // as a separator\n",
    "SAVE_SAMPLE_PROMPT= \"a photo of {SKS} // a painting of {SKS} person by Picasso\" #@param{type: 'string'}\n",
    "if SAVE_SAMPLE_PROMPT==\"\":\n",
    "  SAVE_SAMPLE_PROMPT=None\n",
    "else:\n",
    "  SAVE_SAMPLE_PROMPT=SAVE_SAMPLE_PROMPT.replace(\"{SKS}\",INSTANCE_TOKEN)\n",
    "\n",
    "#@markdown The negative prompt, on the other hand, applies to all SAVE_SAMPLE_PROMPTs\n",
    "SAVE_SAMPLE_NEGATIVE_PROMPT=\"border\" #@param{type: 'string'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sWFt9CCYkMO"
   },
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WfL1DJnZYr-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 194067), started 0:47:47 ago. (Use '!kill 194067' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cd7b681a0a31c87e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cd7b681a0a31c87e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title ## (optional) Tensorboard visualization of loss and learning rate\n",
    "#@markdown Once the Tensorboard panel is launched (takes a good 10 seconds), click on the gear icon in upper right, and check Reload data. Then, after launching training in the next cell, click on TIME SERIES in upper left to see updates.\n",
    "#%load_ext tensorboard\n",
    "!rm -rf content/logs\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $LOGGING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Zim-xlhbYlej"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n",
      "Description:\tUbuntu 18.04.5 LTS\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mdiffusers==0.11.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mlora-diffusion @ file:///home/nas4_user/saemeechoi/course/AdvancedDeepLearning/notebooks/content/lora\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mtorchvision==0.15.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mtransformers==4.25.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mxformers==0.0.23\n",
      "\n",
      "Copy-and-paste the text below in your GitHub issue\n",
      "\n",
      "- `Accelerate` version: 0.15.0\n",
      "- Platform: Linux-4.15.0-175-generic-x86_64-with-glibc2.27\n",
      "- Python version: 3.9.18\n",
      "- Numpy version: 1.26.2\n",
      "- PyTorch version (GPU?): 2.0.1 (True)\n",
      "- `Accelerate` default config:\n",
      "\tNot found\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.0.1)\n",
      "    Python  3.9.18 (you have 3.9.18)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "safety_checker/pytorch_model.fp16.safetensors not found\n",
      "Fetching 29 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 8099.27it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Generating class images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 719/719 [1:17:33<00:00,  6.47s/it]\n",
      "Could not enable memory efficient attention. Make sure xformers is installed correctly and a GPU is available: No operator found for `memory_efficient_attention_forward` with inputs:\n",
      "     query       : shape=(1, 2, 1, 40) (torch.float32)\n",
      "     key         : shape=(1, 2, 1, 40) (torch.float32)\n",
      "     value       : shape=(1, 2, 1, 40) (torch.float32)\n",
      "     attn_bias   : <class 'NoneType'>\n",
      "     p           : 0.0\n",
      "`decoderF` is not supported because:\n",
      "    xFormers wasn't build with CUDA support\n",
      "    attn_bias type is <class 'NoneType'>\n",
      "    operator wasn't built - see `python -m xformers.info` for more info\n",
      "`flshattF@0.0.0` is not supported because:\n",
      "    xFormers wasn't build with CUDA support\n",
      "    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n",
      "    operator wasn't built - see `python -m xformers.info` for more info\n",
      "`tritonflashattF` is not supported because:\n",
      "    xFormers wasn't build with CUDA support\n",
      "    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n",
      "    operator wasn't built - see `python -m xformers.info` for more info\n",
      "    triton is not available\n",
      "    Only work on pre-MLIR triton for now\n",
      "`cutlassF` is not supported because:\n",
      "    xFormers wasn't build with CUDA support\n",
      "    operator wasn't built - see `python -m xformers.info` for more info\n",
      "`smallkF` is not supported because:\n",
      "    max(query.shape[-1] != value.shape[-1]) > 32\n",
      "    xFormers wasn't build with CUDA support\n",
      "    operator wasn't built - see `python -m xformers.info` for more info\n",
      "    unsupported embed per head: 40\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/bitsandbytes/cuda_setup/paths.py:93: UserWarning: /home/nas4_user/saemeechoi/anaconda3/envs/tokenflow did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n",
      "/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/nas4_user/saemeechoi/course/AdvancedDeepLearning/notebooks/FineTuning_colab-Copy1.ipynb')}\n",
      "  warn(\n",
      "/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/1000065/vscode-ipc-8ce310f1-0fcc-4fe7-94f5-9c8d791b4172.sock')}\n",
      "  warn(\n",
      "/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 110\n",
      "CUDA SETUP: Loading binary /home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda110.so...\n",
      "***** Running training *****\n",
      "  Num examples = 2219\n",
      "  Num batches each epoch = 2219\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "Steps:   0%|                                          | 0/10000 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/nas4_user/saemeechoi/course/AdvancedDeepLearning/notebooks/content/Dreambooth/train.py\", line 1112, in <module>\n",
      "    main(args)\n",
      "  File \"/home/nas4_user/saemeechoi/course/AdvancedDeepLearning/notebooks/content/Dreambooth/train.py\", line 1005, in main\n",
      "    for step, batch in enumerate(train_dataloader):\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/accelerate/data_loader.py\", line 375, in __iter__\n",
      "    current_batch = next(dataloader_iter)\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/torch/_utils.py\", line 644, in reraise\n",
      "    raise exception\n",
      "ZeroDivisionError: Caught ZeroDivisionError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/nas4_user/saemeechoi/course/AdvancedDeepLearning/notebooks/content/Dreambooth/train.py\", line 398, in __getitem__\n",
      "    image_path = self.instance_images_path[index % self.num_instance_images]\n",
      "ZeroDivisionError: integer division or modulo by zero\n",
      "\n",
      "Steps:   0%|                                          | 0/10000 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 1104, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 567, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/home/nas4_user/saemeechoi/anaconda3/envs/tokenflow/bin/python', 'content/Dreambooth/train.py', '--lora_rank=4', '--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5', '--pretrained_vae_name_or_path=', '--instance_data_dir=content/InstanceImages', '--class_data_dir=content/RegularizationImages', '--output_dir=content/models/', '--logging_dir=content/logs/', '--log_gpu', '--with_prior_preservation', '--prior_loss_weight=1.0', '--instance_prompt=a photo of raretoken person', '--class_prompt=a photo of a person', '--conditioning_dropout_prob=0.0', '--unconditional_prompt= ', '--seed=1275017', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--gradient_checkpointing', '--mixed_precision=fp16', '--use_8bit_adam', '--adam_beta1=0.9', '--adam_beta2=0.999', '--adam_weight_decay=0.01', '--adam_epsilon=1e-08', '--learning_rate=2e-06', '--learning_rate_text=5e-05', '--lr_scheduler=cosine', '--lr_warmup_steps=50', '--lr_cosine_num_cycles=5', '--ema_inv_gamma=1.0', '--ema_power=0.75', '--ema_min_value=0', '--ema_max_value=0.9999', '--max_train_steps=10000', '--num_class_images=1500', '--sample_batch_size=1', '--save_min_steps=500', '--save_interval=500', '--n_save_sample=2', '--save_sample_prompt=a photo of raretoken // a painting of raretoken person by Picasso', '--save_sample_negative_prompt=border', '--augment_center_crop']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "#@title ## Launch training\n",
    "!lsb_release -a | grep Description\n",
    "!pip freeze | grep diffusers\n",
    "!pip freeze | grep lora-diffusion\n",
    "!pip freeze | grep torchvision\n",
    "!pip freeze | grep transformers\n",
    "!pip freeze | grep xformers\n",
    "!accelerate env\n",
    "\n",
    "!accelerate launch \\\n",
    "    --mixed_precision=$MIXED_PRECISION \\\n",
    "    --num_machines=1 \\\n",
    "    --num_processes=1 \\\n",
    "    --dynamo_backend=\"no\" \\\n",
    "    content/Dreambooth/train.py \\\n",
    "    $USE_LORA_FLAG \\\n",
    "    --lora_rank=$LORA_RANK \\\n",
    "    $TRAIN_TEXT_ENCODER_FLAG \\\n",
    "    --pretrained_model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
    "    --pretrained_vae_name_or_path=$VAE_NAME_OR_PATH \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --class_data_dir=\"$CLASS_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --logging_dir=\"$LOGGING_DIR\" \\\n",
    "    $LOG_GPU_FLAG \\\n",
    "    $ENABLE_PRIOR_PRESERVATION_FLAG \\\n",
    "    --prior_loss_weight=$PRIOR_LOSS_WEIGHT \\\n",
    "    --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
    "    --class_prompt=\"$CLASS_PROMPT\" \\\n",
    "    $USE_IMAGE_CAPTIONS_FLAG \\\n",
    "    --conditioning_dropout_prob=$CONDITIONING_DROPOUT_PROB \\\n",
    "    --unconditional_prompt=\"$UNCONDITIONAL_PROMPT\" \\\n",
    "    --seed=$SEED \\\n",
    "    --resolution=$RESOLUTION \\\n",
    "    --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "    --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "    $GRADIENT_CHECKPOINTING_FLAG \\\n",
    "    --mixed_precision=$MIXED_PRECISION \\\n",
    "    $USE_8BIT_ADAM_FLAG \\\n",
    "    --adam_beta1=$ADAM_BETA1 \\\n",
    "    --adam_beta2=$ADAM_BETA2 \\\n",
    "    --adam_weight_decay=$ADAM_WEIGHT_DECAY \\\n",
    "    --adam_epsilon=$ADAM_EPSILON \\\n",
    "    --learning_rate=$LR \\\n",
    "    --learning_rate_text=$LR_TEXT_ENCODER \\\n",
    "    --lr_scheduler=$LR_SCHEDULE \\\n",
    "    --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
    "    --lr_cosine_num_cycles=$LR_COSINE_NUM_CYCLES \\\n",
    "    $USE_EMA_FLAG \\\n",
    "    --ema_inv_gamma=$EMA_INV_GAMMA \\\n",
    "    --ema_power=$EMA_POWER \\\n",
    "    --ema_min_value=$EMA_MIN_VALUE \\\n",
    "    --ema_max_value=$EMA_MAX_VALUE \\\n",
    "    --max_train_steps=$STEPS \\\n",
    "    --num_class_images=$MIN_NUM_CLASS_IMAGES \\\n",
    "    --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
    "    --save_min_steps=$START_SAVING_FROM_STEP \\\n",
    "    --save_interval=$SAVE_CHECKPOINT_EVERY \\\n",
    "    --n_save_sample=$N_SAVE_SAMPLES \\\n",
    "    --save_sample_prompt=\"$SAVE_SAMPLE_PROMPT\" \\\n",
    "    --save_sample_negative_prompt=\"$SAVE_SAMPLE_NEGATIVE_PROMPT\" \\\n",
    "    $AUGMENT_MIN_RESOLUTION_FLAG \\\n",
    "    $AUGMENT_CENTER_CROP_FLAG \\\n",
    "    $AUGMENT_HFLIP_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9tudAPlex_2"
   },
   "source": [
    "# Do inference with trained model(s)\n",
    "\n",
    "Cells in this section can be run to generate grids of images using the trained model(s). I find this useful for probing overtraining, concept bleeding, quality, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-3nj_hjwGFCf"
   },
   "outputs": [],
   "source": [
    "#@title Some imports and utility functions\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline, DPMSolverMultistepScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "from lora_diffusion import monkeypatch_lora, tune_lora_scale\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def get_pipeline(model_name_or_path,\n",
    "                 vae_name_or_path=None,\n",
    "                 text_encoder_name_or_path=None,\n",
    "                 feature_extractor_name_or_path=None,\n",
    "                 revision=\"fp16\"):\n",
    "    #scheduler = DPMSolverMultistepScheduler.from_pretrained(model_name_or_path, subfolder=\"scheduler\")\n",
    "    scheduler = DPMSolverMultistepScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000,\n",
    "        trained_betas=None,\n",
    "        prediction_type=\"epsilon\",\n",
    "        thresholding=False,\n",
    "        algorithm_type=\"dpmsolver++\",\n",
    "        solver_type=\"midpoint\",\n",
    "        lower_order_final=True,\n",
    "    )\n",
    "\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        custom_pipeline=\"lpw_stable_diffusion\",\n",
    "        safety_checker=None,\n",
    "        revision=revision,\n",
    "        scheduler=scheduler,\n",
    "        vae=AutoencoderKL.from_pretrained(\n",
    "            vae_name_or_path or model_name_or_path,\n",
    "            subfolder=None if vae_name_or_path else \"vae\",\n",
    "            revision=None if vae_name_or_path else revision,\n",
    "            torch_dtype=torch.float16,\n",
    "        ),\n",
    "        feature_extractor=feature_extractor_name_or_path,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    #https://github.com/huggingface/diffusers/issues/1552\n",
    "    #pipe.enable_attention_slicing()\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    return pipe\n",
    "\n",
    "# Monkey patch LoRA pt files\n",
    "# Returns pipeline\n",
    "def get_lora_pipeline(model_dir, scale_unet=1.0, scale_text_encoder=1.0):\n",
    "    # Load untrained original model\n",
    "    pipe = get_pipeline(MODEL_NAME_OR_PATH, vae_name_or_path=VAE_NAME_OR_PATH)\n",
    "\n",
    "    print('Monkey patching unet pt file')\n",
    "    monkeypatch_lora(pipe.unet, torch.load(os.path.join(model_dir, \"lora_unet.pt\")))\n",
    "\n",
    "    print('Monkey patching text encoder pt file')\n",
    "    monkeypatch_lora(pipe.text_encoder, torch.load(os.path.join(model_dir, \"lora_text_encoder.pt\")), target_replace_module=[\"CLIPAttention\"])\n",
    "\n",
    "    tune_lora_scale(pipe.unet, scale_unet)\n",
    "    tune_lora_scale(pipe.text_encoder, scale_text_encoder)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def get_config(filename=None,\n",
    "               save_dir=None,\n",
    "               prompt=None, negative_prompt=None,\n",
    "               seeds=None,\n",
    "               num_samples=4,\n",
    "               width=512, height=512,\n",
    "               inference_steps=20,\n",
    "               guidance_scale=7.5,\n",
    "               ):\n",
    "    if filename==None:\n",
    "        num_prompts = len(prompt)\n",
    "        if seeds==None:\n",
    "            seeds = []\n",
    "            # fixed value seeds for easier comparision betwen subsequent runs/config files\n",
    "            for i in range(num_samples):\n",
    "                seeds.append(i * 1000000)\n",
    "        else:\n",
    "            num_samples = len(seeds)\n",
    "\n",
    "        tag = ''.join(random.choice(string.ascii_letters) for _ in range(8))\n",
    "        config = {\n",
    "            \"tag\": tag,\n",
    "            \"prompt\": prompt,\n",
    "            \"negative_prompt\": negative_prompt,\n",
    "            \"num_prompts\": num_prompts,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"seeds\": seeds,\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"inference_steps\": inference_steps,\n",
    "            \"guidance_scale\": guidance_scale,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(save_dir, \"config_\"+tag+\".json\"), \"w\") as outfile:\n",
    "            json.dump(config, outfile)\n",
    "    else:\n",
    "        f = open(filename)\n",
    "        config = json.load(f)\n",
    "\n",
    "    return config\n",
    "\n",
    "def get_images(pipe, sample_config, device=\"cuda\"):\n",
    "    generator = torch.Generator(\"cuda\")\n",
    "    with torch.autocast(device):\n",
    "        num_cfg = len(sample_config['guidance_scale'])\n",
    "        # Loop in order to use defined seed for each image in a batch\n",
    "        all_images = []\n",
    "        for i in range(sample_config['num_samples']):\n",
    "        #for _ in sample_config['num_samples']:\n",
    "            for cfg in sample_config['guidance_scale']:\n",
    "                # Manually generate latent\n",
    "                seed = sample_config['seeds'][i]\n",
    "                generator = generator.manual_seed(seed)\n",
    "                latent = torch.randn(\n",
    "                    (1, pipe.unet.in_channels, sample_config['height'] // 8, sample_config['width'] // 8),\n",
    "                    generator = generator,\n",
    "                    device = device\n",
    "                )\n",
    "                images = pipe(sample_config['prompt'],\n",
    "                    negative_prompt=sample_config['negative_prompt'],\n",
    "                    num_inference_steps=int(sample_config['inference_steps']),\n",
    "                    guidance_scale=cfg,\n",
    "                    latents=latent.repeat(sample_config['num_prompts'], 1, 1, 1),\n",
    "                ).images\n",
    "                all_images.extend(images)\n",
    "\n",
    "    grid = image_grid(all_images, rows=num_cfg*sample_config['num_samples'], cols=sample_config['num_prompts'])\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w3Xv0Zks-fCp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/models/500', '/content/models/1000', '/content/models/2500']\n"
     ]
    }
   ],
   "source": [
    "#@title Specify which models to do inference with\n",
    "model_list = [os.path.join(OUTPUT_DIR,'500'),\n",
    "              os.path.join(OUTPUT_DIR,'1000'),\n",
    "              os.path.join(OUTPUT_DIR,'2500'),\n",
    "              ]\n",
    "\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub3iLhz0aBho"
   },
   "outputs": [],
   "source": [
    "#@title Generate or load a configuration for inference\n",
    "\n",
    "config_name = None\n",
    "#config_name = os.path.join(OUTPUT_DIR, \"config_ZMasiqkP.json\")\n",
    "\n",
    "if config_name is None:\n",
    "    num_samples = 6\n",
    "    prompt = [\"photo of a cat\",\n",
    "              \"photo of a person\",\n",
    "              \"close-up studio portrait photo of Keanu Reeves, film, detail, studio lighting\",\n",
    "              \"close-up studio portrait photo of {SKS} person, film, detail, studio lighting\",\n",
    "              \"beautiful white (marble:1.1) bust of {SKS} person, highly detailed\",\n",
    "              \"oil painting of {SKS} person on the beach\",\n",
    "    ]\n",
    "    negative_prompt = \"hands, nude, nudity, duplicate, frame, border\"\n",
    "    guidance_scale = [1.0, 3.0, 7.0, 15.0]\n",
    "\n",
    "    config = get_config(save_dir=OUTPUT_DIR,\n",
    "                        prompt=prompt, negative_prompt=negative_prompt,\n",
    "                        num_samples=num_samples,\n",
    "                        width=512, height=512,\n",
    "                        inference_steps=20, guidance_scale=guidance_scale\n",
    "                        )\n",
    "else:\n",
    "    config = get_config(filename=config_name)\n",
    "\n",
    "config['prompt'] = [sub.replace('{SKS}', INSTANCE_TOKEN) for sub in config['prompt']]\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMMHcQ55_PAb"
   },
   "outputs": [],
   "source": [
    "#@title Infer!\n",
    "\n",
    "LORA_SCALE_UNET = 1.0 #@param {type:\"slider\", min:0.0, max:2.0}\n",
    "LORA_SCALE_TENC = 1.0 #@param {type:\"slider\", min:0.0, max:2.0}\n",
    "\n",
    "for model in model_list:\n",
    "    print(model)\n",
    "    pipe = get_pipeline(model) if not USE_LORA else get_lora_pipeline(model, scale_unet=LORA_SCALE_UNET, scale_text_encoder=LORA_SCALE_TENC)\n",
    "    grid = get_images(pipe, config)\n",
    "    grid.save(os.path.join(OUTPUT_DIR, \"grid_\"+os.path.split(model)[1]+\"_\"+config['tag']+\".jpg\"), quality=90, optimize=True)\n",
    "    del pipe\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq4VAh6EPhja"
   },
   "outputs": [],
   "source": [
    "#@title Generate grids for base model using same config\n",
    "model_name_or_path = MODEL_NAME_OR_PATH #'runwayml/stable-diffusion-v1-5'\n",
    "vae_name_or_path = VAE_NAME_OR_PATH #'stabilityai/sd-vae-ft-mse'\n",
    "pipe = get_pipeline(model_name_or_path, vae_name_or_path=vae_name_or_path)\n",
    "grid = get_images(pipe, config)\n",
    "grid.save(os.path.join(OUTPUT_DIR, \"grid_\"+os.path.split(model_name_or_path)[1]+\"_\"+config['tag']+\".jpg\"), quality=90, optimize=True)\n",
    "\n",
    "del pipe\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2WxeLuIqW2G"
   },
   "source": [
    "# Convert to checkpoint (ckpt) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hX2OSGlOuDD-"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/huggingface/diffusers/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "\n",
    "MODEL_PATH = os.path.join(OUTPUT_DIR, '500')\n",
    "CKPT_PATH = os.path.join(OUTPUT_DIR, '500.ckpt')\n",
    "\n",
    "!python content/convert_diffusers_to_original_stable_diffusion.py \\\n",
    "  --model_path $MODEL_PATH \\\n",
    "  --checkpoint_path $CKPT_PATH \\\n",
    "  --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fE3xIr7lE2f"
   },
   "source": [
    "# Close Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYrrT17slGd5"
   },
   "outputs": [],
   "source": [
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
